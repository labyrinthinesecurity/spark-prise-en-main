{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad5563ba-b8d9-46fd-a485-ceb4735e2c21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dans ce TP, nous considérons des trajets en vélo partagé (similaire au vélib) en Californie. Deux jeux de données sont fournis : l'un qui contient les stations de vélo, l'autre, les trajets à vélo. Les déplacements à vélo se font d'une station à l'autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf7dc796-dd34-42c2-8c22-93f6b85ad47c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Charger le fichier stationData.csv dans un DataFrame station_df et le fichier tripData.csv dans un DataFrame trip_df. Pour chaque Dataframe, il vous faudra demander une inférence des schémas et indiquer que la première ligne est un en-tête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436bbfc0-b90d-4202-8e9f-55099321d9ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/stationData-8.csv</td><td>stationData-8.csv</td><td>5201</td><td>1728312920000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/stationData-8.csv",
         "stationData-8.csv",
         5201,
         1728312920000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /FileStore/tables/stationData-8.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7af2f8ad-32d3-4d66-9862-b0d2c55e15e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Afficher les schémas des 2 DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2e4af7-8c45-4a58-9965-f600074ca648",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_df = spark.read.option(\n",
    "    \"header\", \"true\"\n",
    ").csv(\n",
    "    \"/FileStore/tables/stationData-8.csv\"\n",
    ")\n",
    "trip_df = spark.read.csv(\"/FileStore/tables/tripData-8.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e9fa3f-1865-4837-9309-ad49ac91a327",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- station_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- long: string (nullable = true)\n |-- dockcount: string (nullable = true)\n |-- landmark: string (nullable = true)\n |-- installation: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "station_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b35305b3-5b4b-411a-80dd-9315b3b8c743",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Créer une vue pour chaque DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efee527c-afbe-413d-88f6-d75be200eee3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_df.createOrReplaceTempView(\"station_view\")\n",
    "trip_df.createOrReplaceTempView(\"trip_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb92e5c-797b-4cad-8392-722d513cfb38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Trouver deux façons de calculer le nombre de trajets, l'une en appelant une méthode sur trip_df directement, l'autre en rédigeant une requête SQL de la vue correspondant au DataFrame tripData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50b63fd-c346-4158-bc06-3b8100194f24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354152\n"
     ]
    }
   ],
   "source": [
    "print(trip_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94fc9df-52d2-4bdd-8fef-629c49bbe01e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|  354152|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM trip_view\n",
    "    \"\"\"\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e8bd91-78fc-4248-8066-ac6b3ece7dfb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ecrire une requête permettant de compter le nombre de trajets qui démarrent et se terminent à la même station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6365c23a-ec09-4ff6-a926-3d8e630c1598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   10276|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM trip_view\n",
    "    WHERE StartTerminal=EndTerminal\n",
    "    \"\"\"\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb480a36-196d-4d88-bd35-12b4ab058136",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "On souhaite désormais obtenir l’id des stations associées à ces trajets. Ecrire une requête renvoyant la liste des terminaux concernés ainsi que le nombre de trajets pour chacun de ces terminaux. Trier le résultat par ordre décroissant de nombre de trajets.\n",
    "<br>Exemple de sortie :\n",
    "<br>+--------+--------+\n",
    "<br>|terminal|count(1)|\n",
    "<br>+--------+--------+\n",
    "<br>| 60| 850|\n",
    "<br>| 50| 708|\n",
    "<br>| 35| 348|\n",
    "<br>| 76| 320|\n",
    "<br>| 74| 307|\n",
    "<br>(La station 60 est la plus concernée par ces trajets cycliques, avec 850 de ces trajets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c766cd-b7b6-48bf-a516-89eb0ca26aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n|terminal|count|\n+--------+-----+\n|      60|  850|\n|      50|  708|\n|      35|  348|\n|      76|  320|\n|      74|  307|\n|      39|  296|\n|      61|  280|\n|      67|  277|\n|      71|  268|\n|      70|  260|\n|      28|  254|\n|      48|  248|\n|      54|  230|\n|      69|  227|\n|      42|  213|\n|      73|  200|\n|      57|  197|\n|      64|  194|\n|       3|  189|\n|      72|  181|\n+--------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT StartTerminal as terminal, count(*) as count\n",
    "    FROM trip_view\n",
    "    WHERE StartTerminal=EndTerminal\n",
    "    GROUP BY terminal\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5512115f-f2b2-4fef-a466-0847a7ea61c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dans la requête précédente, nous avons oublié un élément qui nous importe. Nous souhaitons compléter le résultat en indiquant le nombre de docks (dockcount) des stations concernées.\n",
    "<br>Exemple de sortie :\n",
    "<br>+--------+---------+--------+\n",
    "<br>|terminal|dockcount|count(1)|\n",
    "<br>+--------+---------+--------+\n",
    "<br>| 60| 15| 850|\n",
    "<br>| 50| 23| 708|\n",
    "<br>| 35| 11| 348|\n",
    "<br>| 76| 19| 320|\n",
    "<br>| 74| 23| 307|\n",
    "<br>Mettre à jour la requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a2b7f-a932-4575-bdc4-d15593a184f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+\n|terminal|dockcount|count|\n+--------+---------+-----+\n|      60|       15|  850|\n|      50|       23|  708|\n|      35|       11|  348|\n|      76|       19|  320|\n|      74|       23|  307|\n|      39|       19|  296|\n|      61|       27|  280|\n|      67|       27|  277|\n|      71|       19|  268|\n|      70|       19|  260|\n|      28|       23|  254|\n|      48|       15|  248|\n|      54|       15|  230|\n|      69|       23|  227|\n|      42|       15|  213|\n|      73|       15|  200|\n|      57|       15|  197|\n|      64|       15|  194|\n|       3|       15|  189|\n|      72|       23|  181|\n+--------+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT StartTerminal as terminal, dockcount, count(*) as count\n",
    "    FROM trip_view, station_view\n",
    "    WHERE StartTerminal=EndTerminal AND station_id=StartTerminal\n",
    "    GROUP BY terminal, dockcount\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ca6c3d-0c56-4e48-9a52-d5e726699e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rédiger les 2 requêtes précédentes avec le DSL de DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c956dd4-dab0-4c4e-94f8-5c46cf63f004",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b61df3b-6e86-4145-a96d-92cba40f5f3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "res = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT StartTerminal as terminal, dockcount, count(*) as count\n",
    "    FROM trip_view, station_view\n",
    "    WHERE StartTerminal=EndTerminal AND station_id=StartTerminal\n",
    "    GROUP BY terminal, dockcount\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "084c0fe3-26fa-4700-b2df-09b0acde565c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_out_2 = trip_df.filter(\n",
    "    col(\"StartStation\") == col(\"EndStation\")\n",
    ").join(\n",
    "    station_df,\n",
    "    trip_df[\"StartTerminal\"] == station_df[\"station_id\"]\n",
    ").groupBy(\n",
    "    \"StartTerminal\", \"dockcount\"\n",
    ").count(\n",
    ").orderBy(\n",
    "    desc(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234663e5-0ea6-4b00-9af3-1269fcc80a74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+\n|StartTerminal|dockcount|count|\n+-------------+---------+-----+\n|           60|       15|  850|\n|           50|       23|  708|\n|           35|       11|  348|\n|           76|       19|  320|\n|           74|       23|  307|\n|           39|       19|  296|\n|           61|       27|  280|\n|           67|       27|  277|\n|           71|       19|  268|\n|           70|       19|  260|\n|           28|       23|  254|\n|           48|       15|  248|\n|           54|       15|  230|\n|           69|       23|  227|\n|           42|       15|  213|\n|           73|       15|  200|\n|           57|       15|  197|\n|           64|       15|  194|\n|            3|       15|  189|\n|           72|       23|  181|\n+-------------+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c25a1c70-14b9-48dd-9f8d-96e8d6e7eec2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Observer le plan d’exécution des requêtes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b85a3c-55f6-42dc-be19-a0099d957fa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   Sort [count#1083L DESC NULLS LAST], true, 0\n   +- Exchange rangepartitioning(count#1083L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2627]\n      +- Project [StartTerminal#293, dockcount#262, count(1)#1082L AS count#1083L]\n         +- HashAggregate(keys=[StartTerminal#293, dockcount#262], functions=[finalmerge_count(merge count#1088L) AS count(1)#1082L])\n            +- Exchange hashpartitioning(StartTerminal#293, dockcount#262, 200), ENSURE_REQUIREMENTS, [plan_id=2623]\n               +- HashAggregate(keys=[StartTerminal#293, dockcount#262], functions=[merge_count(merge count#1088L) AS count#1088L])\n                  +- Project [StartTerminal#293, dockcount#262, count#1088L]\n                     +- BroadcastHashJoin [StartTerminal#293], [station_id#258], Inner, BuildRight, false, true\n                        :- HashAggregate(keys=[StartTerminal#293], functions=[partial_count(1) AS count#1088L])\n                        :  +- Project [StartTerminal#293]\n                        :     +- Filter (((isnotnull(StartStation#292) AND isnotnull(EndStation#295)) AND (StartStation#292 = EndStation#295)) AND isnotnull(StartTerminal#293))\n                        :        +- FileScan csv [StartStation#292,StartTerminal#293,EndStation#295] Batched: false, DataFilters: [isnotnull(StartStation#292), isnotnull(EndStation#295), (StartStation#292 = EndStation#295), isn..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/tripData-8.csv], PartitionFilters: [], PushedFilters: [IsNotNull(StartStation), IsNotNull(EndStation), IsNotNull(StartTerminal)], ReadSchema: struct<StartStation:string,StartTerminal:string,EndStation:string>\n                        +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=2618]\n                           +- Filter isnotnull(station_id#258)\n                              +- FileScan csv [station_id#258,dockcount#262] Batched: false, DataFilters: [isnotnull(station_id#258)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/stationData-8.csv], PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct<station_id:string,dockcount:string>\n\n\n"
     ]
    }
   ],
   "source": [
    "df_out_2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94ea632-7aec-4da0-8c52-ac7bb7d28652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Sort ['count DESC NULLS LAST], true\n+- Aggregate [StartTerminal#293, dockcount#262], [StartTerminal#293, dockcount#262, count(1) AS count#915L]\n   +- Join Inner, (StartTerminal#293 = station_id#258)\n      :- Filter (StartStation#292 = EndStation#295)\n      :  +- Relation [TripID#289,Duration#290,StartDate#291,StartStation#292,StartTerminal#293,EndDate#294,EndStation#295,EndTerminal#296,Bike##297,SubscriberType#298,ZipCode#299] csv\n      +- Relation [station_id#258,name#259,lat#260,long#261,dockcount#262,landmark#263,installation#264] csv\n\n== Analyzed Logical Plan ==\nStartTerminal: string, dockcount: string, count: bigint\nSort [count#915L DESC NULLS LAST], true\n+- Aggregate [StartTerminal#293, dockcount#262], [StartTerminal#293, dockcount#262, count(1) AS count#915L]\n   +- Join Inner, (StartTerminal#293 = station_id#258)\n      :- Filter (StartStation#292 = EndStation#295)\n      :  +- Relation [TripID#289,Duration#290,StartDate#291,StartStation#292,StartTerminal#293,EndDate#294,EndStation#295,EndTerminal#296,Bike##297,SubscriberType#298,ZipCode#299] csv\n      +- Relation [station_id#258,name#259,lat#260,long#261,dockcount#262,landmark#263,installation#264] csv\n\n== Optimized Logical Plan ==\nSort [count#915L DESC NULLS LAST], true\n+- Project [StartTerminal#293, dockcount#262, count(1)#914L AS count#915L]\n   +- AggregatePart [StartTerminal#293, dockcount#262], [finalmerge_count(merge count#920L) AS count(1)#914L], true\n      +- AggregatePart [StartTerminal#293, dockcount#262], [merge_count(merge count#920L) AS count#920L], false\n         +- Project [StartTerminal#293, dockcount#262, count#920L]\n            +- Join Inner, (StartTerminal#293 = station_id#258)\n               :- AggregatePart [StartTerminal#293], [partial_count(1) AS count#920L], false\n               :  +- Project [StartTerminal#293]\n               :     +- Filter (((isnotnull(StartStation#292) AND isnotnull(EndStation#295)) AND (StartStation#292 = EndStation#295)) AND isnotnull(StartTerminal#293))\n               :        +- Relation [TripID#289,Duration#290,StartDate#291,StartStation#292,StartTerminal#293,EndDate#294,EndStation#295,EndTerminal#296,Bike##297,SubscriberType#298,ZipCode#299] csv\n               +- Project [station_id#258, dockcount#262]\n                  +- Filter isnotnull(station_id#258)\n                     +- Relation [station_id#258,name#259,lat#260,long#261,dockcount#262,landmark#263,installation#264] csv\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ResultQueryStage 3, Statistics(sizeInBytes=3.3 KiB, rowCount=70, ColumnStat: N/A, isRuntime=true)\n   +- Sort [count#915L DESC NULLS LAST], true, 0\n      +- AQEShuffleRead coalesced\n         +- ShuffleQueryStage 2, Statistics(sizeInBytes=3.3 KiB, rowCount=70, ColumnStat: N/A, isRuntime=true)\n            +- Exchange rangepartitioning(count#915L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=2150]\n               +- *(3) Project [StartTerminal#293, dockcount#262, count(1)#914L AS count#915L]\n                  +- *(3) HashAggregate(keys=[StartTerminal#293, dockcount#262], functions=[finalmerge_count(merge count#920L) AS count(1)#914L], output=[StartTerminal#293, dockcount#262, count(1)#914L])\n                     +- AQEShuffleRead coalesced\n                        +- ShuffleQueryStage 1, Statistics(sizeInBytes=13.0 KiB, rowCount=278, ColumnStat: N/A, isRuntime=true)\n                           +- Exchange hashpartitioning(StartTerminal#293, dockcount#262, 200), ENSURE_REQUIREMENTS, [plan_id=2114]\n                              +- *(2) HashAggregate(keys=[StartTerminal#293, dockcount#262], functions=[merge_count(merge count#920L) AS count#920L], output=[StartTerminal#293, dockcount#262, count#920L])\n                                 +- *(2) Project [StartTerminal#293, dockcount#262, count#920L]\n                                    +- *(2) BroadcastHashJoin [StartTerminal#293], [station_id#258], Inner, BuildRight, false, true\n                                       :- *(2) HashAggregate(keys=[StartTerminal#293], functions=[partial_count(1) AS count#920L], output=[StartTerminal#293, count#920L])\n                                       :  +- *(2) Project [StartTerminal#293]\n                                       :     +- *(2) Filter (((isnotnull(StartStation#292) AND isnotnull(EndStation#295)) AND (StartStation#292 = EndStation#295)) AND isnotnull(StartTerminal#293))\n                                       :        +- FileScan csv [StartStation#292,StartTerminal#293,EndStation#295] Batched: false, DataFilters: [isnotnull(StartStation#292), isnotnull(EndStation#295), (StartStation#292 = EndStation#295), isn..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/tripData-8.csv], PartitionFilters: [], PushedFilters: [IsNotNull(StartStation), IsNotNull(EndStation), IsNotNull(StartTerminal)], ReadSchema: struct<StartStation:string,StartTerminal:string,EndStation:string>\n                                       +- ShuffleQueryStage 0, Statistics(sizeInBytes=2.7 KiB, rowCount=70, ColumnStat: N/A, isRuntime=true)\n                                          +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=1935]\n                                             +- *(1) Filter isnotnull(station_id#258)\n                                                +- FileScan csv [station_id#258,dockcount#262] Batched: false, DataFilters: [isnotnull(station_id#258)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/tables/stationData-8.csv], PartitionFilters: [], PushedFilters: [IsNotNull(station_id)], ReadSchema: struct<station_id:string,dockcount:string>\n\n"
     ]
    }
   ],
   "source": [
    "df_out.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd90d40e-4285-4b15-bcf0-3ec8c1a86a77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3959968822295793,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-prise-en-main_enonce_spark-sql-dsl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
